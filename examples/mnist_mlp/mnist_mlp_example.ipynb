{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "1s - loss: 0.4320 - acc: 0.8734 - val_loss: 0.1685 - val_acc: 0.9499\n",
      "Epoch 2/20\n",
      "1s - loss: 0.1827 - acc: 0.9457 - val_loss: 0.1177 - val_acc: 0.9646\n",
      "Epoch 3/20\n",
      "1s - loss: 0.1366 - acc: 0.9585 - val_loss: 0.0923 - val_acc: 0.9713\n",
      "Epoch 4/20\n",
      "1s - loss: 0.1107 - acc: 0.9666 - val_loss: 0.0825 - val_acc: 0.9748\n",
      "Epoch 5/20\n",
      "1s - loss: 0.0951 - acc: 0.9704 - val_loss: 0.0809 - val_acc: 0.9758\n",
      "Epoch 6/20\n",
      "1s - loss: 0.0812 - acc: 0.9745 - val_loss: 0.0714 - val_acc: 0.9774\n",
      "Epoch 7/20\n",
      "1s - loss: 0.0723 - acc: 0.9772 - val_loss: 0.0706 - val_acc: 0.9775\n",
      "Epoch 8/20\n",
      "1s - loss: 0.0683 - acc: 0.9785 - val_loss: 0.0679 - val_acc: 0.9790\n",
      "Epoch 9/20\n",
      "1s - loss: 0.0601 - acc: 0.9809 - val_loss: 0.0674 - val_acc: 0.9794\n",
      "Epoch 10/20\n",
      "1s - loss: 0.0548 - acc: 0.9827 - val_loss: 0.0692 - val_acc: 0.9794\n",
      "Epoch 11/20\n",
      "1s - loss: 0.0513 - acc: 0.9831 - val_loss: 0.0668 - val_acc: 0.9797\n",
      "Epoch 12/20\n",
      "1s - loss: 0.0476 - acc: 0.9849 - val_loss: 0.0689 - val_acc: 0.9810\n",
      "Epoch 13/20\n",
      "1s - loss: 0.0455 - acc: 0.9852 - val_loss: 0.0724 - val_acc: 0.9794\n",
      "Epoch 14/20\n",
      "1s - loss: 0.0433 - acc: 0.9854 - val_loss: 0.0670 - val_acc: 0.9806\n",
      "Epoch 15/20\n",
      "1s - loss: 0.0414 - acc: 0.9860 - val_loss: 0.0644 - val_acc: 0.9820\n",
      "Epoch 16/20\n",
      "1s - loss: 0.0399 - acc: 0.9869 - val_loss: 0.0752 - val_acc: 0.9779\n",
      "Epoch 17/20\n",
      "1s - loss: 0.0375 - acc: 0.9876 - val_loss: 0.0666 - val_acc: 0.9817\n",
      "Epoch 18/20\n",
      "1s - loss: 0.0356 - acc: 0.9883 - val_loss: 0.0666 - val_acc: 0.9813\n",
      "Epoch 19/20\n",
      "1s - loss: 0.0321 - acc: 0.9889 - val_loss: 0.0735 - val_acc: 0.9798\n",
      "Epoch 20/20\n",
      "1s - loss: 0.0327 - acc: 0.9885 - val_loss: 0.0712 - val_acc: 0.9809\n",
      "Test score: 0.0711919080993\n",
      "Test accuracy: 0.9809\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', class_mode='categorical')\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "layer_name_dict = {\n",
    "    'Dense': 'denseLayer',\n",
    "    'Dropout': 'dropoutLayer',\n",
    "    'Flatten': 'flattenLayer',\n",
    "    'Embedding': 'embeddingLayer',\n",
    "    'BatchNormalization': 'batchNormalizationLayer',\n",
    "    'LeakyReLU': 'leakyReLULayer',\n",
    "    'PReLU': 'parametricReLULayer',\n",
    "    'ParametricSoftplus': 'parametricSoftplusLayer',\n",
    "    'ThresholdedLinear': 'thresholdedLinearLayer',\n",
    "    'ThresholdedReLu': 'thresholdedReLuLayer',\n",
    "    'LSTM': 'rLSTMLayer',\n",
    "    'GRU': 'rGRULayer',\n",
    "    'JZS1': 'rJZS1Layer',\n",
    "    'JZS2': 'rJZS2Layer',\n",
    "    'JZS3': 'rJZS3Layer',\n",
    "    'Convolution2D': 'convolution2DLayer',\n",
    "    'MaxPooling2D': 'maxPooling2DLayer'\n",
    "}\n",
    "\n",
    "layer_params_dict = {\n",
    "    'Dense': ['weights', 'activation'],\n",
    "    'Dropout': ['p'],\n",
    "    'Flatten': [],\n",
    "    'Embedding': ['weights'],\n",
    "    'BatchNormalization': ['weights', 'epsilon'],\n",
    "    'LeakyReLU': ['alpha'],\n",
    "    'PReLU': ['weights'],\n",
    "    'ParametricSoftplus': ['weights'],\n",
    "    'ThresholdedLinear': ['theta'],\n",
    "    'ThresholdedReLu': ['theta'],\n",
    "    'LSTM': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'GRU': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS1': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS2': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS3': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'Convolution2D': ['weights', 'nb_filter', 'nb_row', 'nb_col', 'border_mode', 'subsample', 'activation'],\n",
    "    'MaxPooling2D': ['pool_size', 'stride', 'ignore_border']\n",
    "}\n",
    "\n",
    "layer_weights_dict = {\n",
    "    'Dense': ['W', 'b'],\n",
    "    'Embedding': ['E'],\n",
    "    'BatchNormalization': ['gamma', 'beta', 'mean', 'std'],\n",
    "    'PReLU': ['alphas'],\n",
    "    'ParametricSoftplus': ['alphas', 'betas'],\n",
    "    'LSTM': ['W_xi', 'W_hi', 'b_i', 'W_xc', 'W_hc', 'b_c', 'W_xf', 'W_hf', 'b_f', 'W_xo', 'W_ho', 'b_o'],\n",
    "    'GRU': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'JZS1': ['W_xz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS2': ['W_xz', 'W_hz', 'b_z', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS3': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'Convolution2D': ['W', 'b']\n",
    "}\n",
    "\n",
    "def serialize(model_json_file, weights_hdf5_file, save_filepath, compress):\n",
    "    with open(model_json_file, 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    weights_file = h5py.File(weights_hdf5_file, 'r')\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    num_activation_layers = 0\n",
    "    for k, layer in enumerate(model_metadata['layers']):\n",
    "        if layer['name'] == 'Activation':\n",
    "            num_activation_layers += 1\n",
    "            prev_layer_name = model_metadata['layers'][k-1]['name']\n",
    "            idx_activation = layer_params_dict[prev_layer_name].index('activation')\n",
    "            layers[k-num_activation_layers]['parameters'][idx_activation] = layer['activation']\n",
    "            continue\n",
    "\n",
    "        layer_params = []\n",
    "\n",
    "        for param in layer_params_dict[layer['name']]:\n",
    "            if param == 'weights':\n",
    "                layer_weights = list(weights_file.get('layer_{}'.format(k)))\n",
    "                weights = {}\n",
    "                weight_names = layer_weights_dict[layer['name']]\n",
    "                for name, w in zip(weight_names, layer_weights):\n",
    "                    weights[name] = weights_file.get('layer_{}/{}'.format(k, w)).value.tolist()\n",
    "                layer_params.append(weights)\n",
    "            else:\n",
    "                layer_params.append(layer[param])\n",
    "\n",
    "        layers.append({\n",
    "            'layerName': layer_name_dict[layer['name']],\n",
    "            'parameters': layer_params\n",
    "        })\n",
    "\n",
    "    if compress:\n",
    "        with gzip.open(save_filepath, 'wb') as f:\n",
    "            f.write(json.dumps(layers).encode('utf8'))\n",
    "    else:\n",
    "        with open(save_filepath, 'w') as f:\n",
    "            json.dump(layers, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "model_metadata = json.loads(model.to_json())\n",
    "\n",
    "with open('mnist_mlp_keras_model.json', 'w') as f:\n",
    "    json.dump(model_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_mode': 'categorical',\n",
       " 'layers': [{'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': 784,\n",
       "   'input_shape': [784],\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 128},\n",
       "  {'activation': 'relu', 'beta': 0.1, 'name': 'Activation', 'target': 0},\n",
       "  {'name': 'Dropout', 'p': 0.2},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 128},\n",
       "  {'activation': 'relu', 'beta': 0.1, 'name': 'Activation', 'target': 0},\n",
       "  {'name': 'Dropout', 'p': 0.2},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 10},\n",
       "  {'activation': 'softmax', 'beta': 0.1, 'name': 'Activation', 'target': 0}],\n",
       " 'loss': 'categorical_crossentropy',\n",
       " 'name': 'Sequential',\n",
       " 'optimizer': {'beta_1': 0.9,\n",
       "  'beta_2': 0.999,\n",
       "  'epsilon': 1e-08,\n",
       "  'lr': 0.0010000000474974513,\n",
       "  'name': 'Adam'},\n",
       " 'theano_mode': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] mnist_mlp_keras_weights.hdf5 already exists - overwrite? [y/n]y\n",
      "[TIP] Next time specify overwrite=True in save_weights!\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('mnist_mlp_keras_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serialize('mnist_mlp_keras_model.json', \n",
    "          'mnist_mlp_keras_weights.hdf5', \n",
    "          'mnist_mlp_model_params.json.gz', \n",
    "          True)\n",
    "serialize('mnist_mlp_keras_model.json', \n",
    "          'mnist_mlp_keras_weights.hdf5', \n",
    "          'mnist_mlp_model_params.json', \n",
    "          False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randidx = np.random.randint(0, X_test.shape[0], size=2000)\n",
    "X_rand = X_test[randidx, :]\n",
    "y_rand = y_test[randidx]\n",
    "\n",
    "with gzip.open('sample_data.json.gz', 'wb') as f:\n",
    "    f.write(json.dumps({'data': X_rand.tolist(), 'labels': y_rand.tolist()}).encode('utf8'))\n",
    "with open('sample_data.json', 'w') as f:\n",
    "    json.dump({'data': X_rand.tolist(), 'labels': y_rand.tolist()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99999642e-01,   1.93592386e-12,   3.22102466e-07,\n",
       "          5.10120501e-10,   8.43486322e-11,   1.41102746e-11,\n",
       "          1.82862205e-08,   8.58895177e-10,   3.75560418e-08,\n",
       "          2.18974616e-09]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_rand[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
