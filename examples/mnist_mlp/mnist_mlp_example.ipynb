{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "1s - loss: 0.4275 - acc: 0.8746 - val_loss: 0.1688 - val_acc: 0.9484\n",
      "Epoch 2/20\n",
      "1s - loss: 0.1852 - acc: 0.9454 - val_loss: 0.1147 - val_acc: 0.9648\n",
      "Epoch 3/20\n",
      "1s - loss: 0.1381 - acc: 0.9585 - val_loss: 0.1054 - val_acc: 0.9665\n",
      "Epoch 4/20\n",
      "1s - loss: 0.1129 - acc: 0.9656 - val_loss: 0.0845 - val_acc: 0.9731\n",
      "Epoch 5/20\n",
      "1s - loss: 0.0946 - acc: 0.9704 - val_loss: 0.0788 - val_acc: 0.9748\n",
      "Epoch 6/20\n",
      "1s - loss: 0.0824 - acc: 0.9744 - val_loss: 0.0708 - val_acc: 0.9772\n",
      "Epoch 7/20\n",
      "1s - loss: 0.0744 - acc: 0.9770 - val_loss: 0.0730 - val_acc: 0.9761\n",
      "Epoch 8/20\n",
      "1s - loss: 0.0674 - acc: 0.9788 - val_loss: 0.0679 - val_acc: 0.9782\n",
      "Epoch 9/20\n",
      "1s - loss: 0.0623 - acc: 0.9799 - val_loss: 0.0678 - val_acc: 0.9806\n",
      "Epoch 10/20\n",
      "1s - loss: 0.0559 - acc: 0.9818 - val_loss: 0.0685 - val_acc: 0.9787\n",
      "Epoch 11/20\n",
      "1s - loss: 0.0507 - acc: 0.9840 - val_loss: 0.0683 - val_acc: 0.9801\n",
      "Epoch 12/20\n",
      "1s - loss: 0.0511 - acc: 0.9831 - val_loss: 0.0698 - val_acc: 0.9790\n",
      "Epoch 13/20\n",
      "1s - loss: 0.0468 - acc: 0.9847 - val_loss: 0.0642 - val_acc: 0.9813\n",
      "Epoch 14/20\n",
      "1s - loss: 0.0432 - acc: 0.9861 - val_loss: 0.0668 - val_acc: 0.9807\n",
      "Epoch 15/20\n",
      "1s - loss: 0.0409 - acc: 0.9866 - val_loss: 0.0682 - val_acc: 0.9801\n",
      "Epoch 16/20\n",
      "1s - loss: 0.0388 - acc: 0.9869 - val_loss: 0.0714 - val_acc: 0.9788\n",
      "Epoch 17/20\n",
      "1s - loss: 0.0384 - acc: 0.9877 - val_loss: 0.0662 - val_acc: 0.9813\n",
      "Epoch 18/20\n",
      "1s - loss: 0.0339 - acc: 0.9889 - val_loss: 0.0705 - val_acc: 0.9800\n",
      "Epoch 19/20\n",
      "1s - loss: 0.0346 - acc: 0.9883 - val_loss: 0.0655 - val_acc: 0.9821\n",
      "Epoch 20/20\n",
      "1s - loss: 0.0321 - acc: 0.9891 - val_loss: 0.0679 - val_acc: 0.9804\n",
      "Test score: 0.0678930273281\n",
      "Test accuracy: 0.9804\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(784, 128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, 128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, 10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', class_mode='categorical')\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "layer_name_dict = {\n",
    "    'Dense': 'denseLayer',\n",
    "    'Dropout': 'dropoutLayer',\n",
    "    'Embedding': 'embeddingLayer',\n",
    "    'BatchNormalization': 'batchNormalizationLayer',\n",
    "    'LSTM': 'rLSTMLayer',\n",
    "    'GRU': 'rGRULayer',\n",
    "    'JZS1': 'rJZS1Layer',\n",
    "    'JZS2': 'rJZS2Layer',\n",
    "    'JZS3': 'rJZS3Layer'\n",
    "}\n",
    "\n",
    "layer_params_dict = {\n",
    "    'Dense': ['weights', 'activation'],\n",
    "    'Dropout': ['p'],\n",
    "    'Embedding': ['weights'],\n",
    "    'BatchNormalization': ['weights', 'epsilon'],\n",
    "    'LSTM': ['weights', 'activation', 'inner_activation'],\n",
    "    'GRU': ['weights', 'activation', 'inner_activation'],\n",
    "    'JZS1': ['weights', 'activation', 'inner_activation'],\n",
    "    'JZS2': ['weights', 'activation', 'inner_activation'],\n",
    "    'JZS3': ['weights', 'activation', 'inner_activation']\n",
    "}\n",
    "\n",
    "layer_weights_dict = {\n",
    "    'Dense': ['W', 'b'],\n",
    "    'Embedding': ['E'],\n",
    "    'BatchNormalization': ['gamma', 'beta', 'mean', 'std'],\n",
    "    'LSTM': ['W_xi', 'W_hi', 'b_i', 'W_xc', 'W_hc', 'b_c', 'W_xf', 'W_hf', 'b_f', 'W_xo', 'W_ho', 'b_o'],\n",
    "    'GRU': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'JZS1': ['W_xz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS2': ['W_xz', 'W_hz', 'b_z', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS3': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h']\n",
    "}\n",
    "\n",
    "def serialize(model_json_file, weights_hdf5_file, save_filepath, compress):\n",
    "    with open(model_json_file, 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    weights_file = h5py.File(weights_hdf5_file, 'r')\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    num_activation_layers = 0\n",
    "    for k, layer in enumerate(model_metadata['layers']):\n",
    "        if layer['name'] == 'Activation':\n",
    "            num_activation_layers += 1\n",
    "            prev_layer_name = model_metadata['layers'][k-1]['name']\n",
    "            idx_activation = layer_params_dict[prev_layer_name].index('activation')\n",
    "            layers[k-num_activation_layers]['parameters'][idx_activation] = layer['activation']\n",
    "            continue\n",
    "\n",
    "        layer_params = []\n",
    "\n",
    "        for param in layer_params_dict[layer['name']]:\n",
    "            if param == 'weights':\n",
    "                layer_weights = list(weights_file.get('layer_{}'.format(k)))\n",
    "                weights = {}\n",
    "                weight_names = layer_weights_dict[layer['name']]\n",
    "                for name, w in zip(weight_names, layer_weights):\n",
    "                    weights[name] = weights_file.get('layer_{}/{}'.format(k, w)).value.tolist()\n",
    "                layer_params.append(weights)\n",
    "            else:\n",
    "                layer_params.append(layer[param])\n",
    "\n",
    "        layers.append({\n",
    "            'layerName': layer_name_dict[layer['name']],\n",
    "            'parameters': layer_params\n",
    "        })\n",
    "\n",
    "    if compress:\n",
    "        with gzip.open(save_filepath, 'wb') as f:\n",
    "            f.write(json.dumps(layers).encode('utf8'))\n",
    "    else:\n",
    "        with open(save_filepath, 'w') as f:\n",
    "            json.dump(layers, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "model_metadata = json.loads(model.to_json())\n",
    "\n",
    "with open('mnist_mlp_keras_model.json', 'w') as f:\n",
    "    json.dump(model_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_mode': 'categorical',\n",
       " 'layers': [{'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': 784,\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 128},\n",
       "  {'activation': 'relu', 'beta': 0.1, 'name': 'Activation', 'target': 0},\n",
       "  {'name': 'Dropout', 'p': 0.2},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': 128,\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 128},\n",
       "  {'activation': 'relu', 'beta': 0.1, 'name': 'Activation', 'target': 0},\n",
       "  {'name': 'Dropout', 'p': 0.2},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': 128,\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 10},\n",
       "  {'activation': 'softmax', 'beta': 0.1, 'name': 'Activation', 'target': 0}],\n",
       " 'loss': 'categorical_crossentropy',\n",
       " 'name': 'Sequential',\n",
       " 'optimizer': {'beta_1': 0.9,\n",
       "  'beta_2': 0.999,\n",
       "  'epsilon': 1e-08,\n",
       "  'lr': 0.0010000000474974513,\n",
       "  'name': 'Adam'},\n",
       " 'theano_mode': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('mnist_mlp_keras_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serialize('mnist_mlp_keras_model.json', \n",
    "          'mnist_mlp_keras_weights.hdf5', \n",
    "          'mnist_mlp_model_params.json.gz', \n",
    "          True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randidx = np.random.randint(0, X_test.shape[0], size=2000)\n",
    "X_rand = X_test[randidx, :]\n",
    "y_rand = y_test[randidx]\n",
    "\n",
    "with gzip.open('sample_data.json.gz', 'wb') as f:\n",
    "    f.write(json.dumps({'data': X_rand.tolist(), 'labels': y_rand.tolist()}).encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.58268265e-10,   7.93496326e-08,   9.99942243e-01,\n",
       "          5.55623519e-05,   4.10324066e-11,   6.82558321e-09,\n",
       "          6.36588453e-08,   6.11314377e-08,   1.99422857e-06,\n",
       "          3.42049875e-13]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_rand[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
