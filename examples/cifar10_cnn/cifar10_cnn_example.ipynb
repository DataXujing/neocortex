{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ParametricSoftplus\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 3, 32, 32)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_classes = 10\n",
    "nb_epoch = 200\n",
    "data_augmentation = False\n",
    "\n",
    "# shape of the image (SHAPE x SHAPE)\n",
    "shapex, shapey = 32, 32\n",
    "# number of convolutional filters to use at each layer\n",
    "nb_filters = [32, 64]\n",
    "# level of pooling to perform at each layer (POOL x POOL)\n",
    "nb_pool = [2, 2]\n",
    "# level of convolution to perform at each layer (CONV x CONV)\n",
    "nb_conv = [3, 3]\n",
    "# the CIFAR10 images are RGB\n",
    "image_dimensions = 3\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation or normalization\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 41s - loss: 1.6144 - acc: 0.4012 - val_loss: 1.2274 - val_acc: 0.5514\n",
      "Epoch 00000: val_loss improved from inf to 1.22742, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 41s - loss: 1.0954 - acc: 0.6092 - val_loss: 0.8795 - val_acc: 0.6918\n",
      "Epoch 00001: val_loss improved from 1.22742 to 0.87954, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.9240 - acc: 0.6781 - val_loss: 0.8358 - val_acc: 0.7163\n",
      "Epoch 00002: val_loss improved from 0.87954 to 0.83582, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.8137 - acc: 0.7197 - val_loss: 0.8461 - val_acc: 0.7152\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.7468 - acc: 0.7424 - val_loss: 0.6712 - val_acc: 0.7796\n",
      "Epoch 00004: val_loss improved from 0.83582 to 0.67119, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.7152 - acc: 0.7536 - val_loss: 0.6634 - val_acc: 0.7780\n",
      "Epoch 00005: val_loss improved from 0.67119 to 0.66343, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.6796 - acc: 0.7659 - val_loss: 0.6026 - val_acc: 0.8010\n",
      "Epoch 00006: val_loss improved from 0.66343 to 0.60262, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.6576 - acc: 0.7748 - val_loss: 0.5906 - val_acc: 0.8041\n",
      "Epoch 00007: val_loss improved from 0.60262 to 0.59062, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.6338 - acc: 0.7818 - val_loss: 0.5881 - val_acc: 0.8053\n",
      "Epoch 00008: val_loss improved from 0.59062 to 0.58811, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.6106 - acc: 0.7899 - val_loss: 0.6009 - val_acc: 0.8057\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.6010 - acc: 0.7927 - val_loss: 0.6039 - val_acc: 0.8030\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5875 - acc: 0.7978 - val_loss: 0.5661 - val_acc: 0.8087\n",
      "Epoch 00011: val_loss improved from 0.58811 to 0.56610, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5841 - acc: 0.7982 - val_loss: 0.5737 - val_acc: 0.8109\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5764 - acc: 0.8022 - val_loss: 0.5804 - val_acc: 0.8099\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5647 - acc: 0.8071 - val_loss: 0.5872 - val_acc: 0.8064\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5549 - acc: 0.8107 - val_loss: 0.5428 - val_acc: 0.8227\n",
      "Epoch 00015: val_loss improved from 0.56610 to 0.54285, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5481 - acc: 0.8121 - val_loss: 0.5516 - val_acc: 0.8207\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5419 - acc: 0.8128 - val_loss: 0.5552 - val_acc: 0.8315\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5456 - acc: 0.8162 - val_loss: 0.5809 - val_acc: 0.8178\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5370 - acc: 0.8172 - val_loss: 0.5637 - val_acc: 0.8153\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5263 - acc: 0.8188 - val_loss: 0.5567 - val_acc: 0.8313\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5289 - acc: 0.8197 - val_loss: 0.6065 - val_acc: 0.8290\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5278 - acc: 0.8213 - val_loss: 0.5433 - val_acc: 0.8281\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5159 - acc: 0.8257 - val_loss: 0.5280 - val_acc: 0.8332\n",
      "Epoch 00023: val_loss improved from 0.54285 to 0.52803, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5069 - acc: 0.8290 - val_loss: 0.5752 - val_acc: 0.8164\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5134 - acc: 0.8277 - val_loss: 0.5800 - val_acc: 0.8150\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5222 - acc: 0.8259 - val_loss: 0.5547 - val_acc: 0.8322\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5115 - acc: 0.8270 - val_loss: 0.5805 - val_acc: 0.8319\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5123 - acc: 0.8261 - val_loss: 0.5619 - val_acc: 0.8246\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5080 - acc: 0.8287 - val_loss: 0.5831 - val_acc: 0.8309\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5161 - acc: 0.8251 - val_loss: 0.6489 - val_acc: 0.8173\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5189 - acc: 0.8273 - val_loss: 0.6268 - val_acc: 0.8250\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5070 - acc: 0.8294 - val_loss: 0.5263 - val_acc: 0.8438\n",
      "Epoch 00032: val_loss improved from 0.52803 to 0.52630, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5017 - acc: 0.8318 - val_loss: 0.5687 - val_acc: 0.8266\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.4979 - acc: 0.8324 - val_loss: 0.5583 - val_acc: 0.8310\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5037 - acc: 0.8337 - val_loss: 0.5396 - val_acc: 0.8366\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.4927 - acc: 0.8344 - val_loss: 0.5550 - val_acc: 0.8312\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.4955 - acc: 0.8349 - val_loss: 0.5323 - val_acc: 0.8403\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5009 - acc: 0.8344 - val_loss: 0.5848 - val_acc: 0.8324\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5016 - acc: 0.8340 - val_loss: 0.6044 - val_acc: 0.8213\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.5006 - acc: 0.8346 - val_loss: 0.6337 - val_acc: 0.8092\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.4946 - acc: 0.8346 - val_loss: 0.5828 - val_acc: 0.8312\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.4955 - acc: 0.8371 - val_loss: 0.5489 - val_acc: 0.8408\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 41s - loss: 0.4870 - acc: 0.8385 - val_loss: 0.5969 - val_acc: 0.8238\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 00043: early stopping\n"
     ]
    }
   ],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(32, 32, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(poolsize=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Convolution2D(64, 32, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(64, 64, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(poolsize=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Convolution2D(128, 64, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(128, 128, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(poolsize=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Convolution2D(256, 128, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(256, 256, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(poolsize=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "# the image dimensions are the original dimensions divided by any pooling\n",
    "# each pixel has a number of filters, determined by the last Convolution2D layer\n",
    "model.add(Dense(256 * (shapex / 2**4) * (shapey / 2**4), 512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', class_mode='categorical')\n",
    "\n",
    "if not data_augmentation:\n",
    "    print(\"Not using data augmentation or normalization\")\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='cifar10_cnn_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "    X_train = X_train.astype(\"float32\")\n",
    "    X_test = X_test.astype(\"float32\")\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    model.fit(X_train, Y_train, \n",
    "              batch_size=batch_size, \n",
    "              nb_epoch=nb_epoch, \n",
    "              show_accuracy=True,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              callbacks=[checkpointer, earlystopping])\n",
    "\n",
    "else:\n",
    "    print(\"Using real time data augmentation\")\n",
    "\n",
    "    # this will do preprocessing and realtime data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied)\n",
    "    datagen.fit(X_train)\n",
    "    \n",
    "    max_test_loss = 1000\n",
    "    current_test_loss = 1000\n",
    "    patience_count = 0\n",
    "\n",
    "    for e in range(nb_epoch):\n",
    "        print('-'*40)\n",
    "        print('Epoch', e)\n",
    "        print('-'*40)\n",
    "        print(\"Training...\")\n",
    "        # batch train with realtime data augmentation\n",
    "        progbar = generic_utils.Progbar(X_train.shape[0])\n",
    "        for X_batch, Y_batch in datagen.flow(X_train, Y_train, batch_size=batch_size):\n",
    "            train_res = model.train_on_batch(X_batch, Y_batch, accuracy=True)\n",
    "            progbar.add(X_batch.shape[0], values=[(\"train loss\", train_res[0]), (\"train acc\", train_res[1])])\n",
    "\n",
    "        print(\"Testing...\")\n",
    "        # test time!\n",
    "        progbar = generic_utils.Progbar(X_test.shape[0])\n",
    "        for X_batch, Y_batch in datagen.flow(X_test, Y_test, batch_size=batch_size):\n",
    "            test_res = model.test_on_batch(X_batch, Y_batch, accuracy=True)\n",
    "            current_test_loss = test_res[0]\n",
    "            progbar.add(X_batch.shape[0], values=[(\"test loss\", test_res[0]), (\"test acc\", test_res[1])])\n",
    "            \n",
    "        print(current_test_loss, max_test_loss)\n",
    "        if current_test_loss < max_test_loss:\n",
    "            print('Saving weights to file.')\n",
    "            model.save_weights('cifar10_cnn_keras_weights.hdf5', overwrite=True)\n",
    "            patience_count = 0\n",
    "            max_test_loss = current_test_loss\n",
    "        elif patience_count > 5:\n",
    "            print('\\nEarly stopping.')\n",
    "            break\n",
    "        else:\n",
    "            patience_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "layer_name_dict = {\n",
    "    'Dense': 'denseLayer',\n",
    "    'Dropout': 'dropoutLayer',\n",
    "    'Flatten': 'flattenLayer',\n",
    "    'Embedding': 'embeddingLayer',\n",
    "    'BatchNormalization': 'batchNormalizationLayer',\n",
    "    'LeakyReLU': 'leakyReLULayer',\n",
    "    'PReLU': 'parametricReLULayer',\n",
    "    'ParametricSoftplus': 'parametricSoftplusLayer',\n",
    "    'ThresholdedLinear': 'thresholdedLinearLayer',\n",
    "    'ThresholdedReLu': 'thresholdedReLuLayer',\n",
    "    'LSTM': 'rLSTMLayer',\n",
    "    'GRU': 'rGRULayer',\n",
    "    'JZS1': 'rJZS1Layer',\n",
    "    'JZS2': 'rJZS2Layer',\n",
    "    'JZS3': 'rJZS3Layer',\n",
    "    'Convolution2D': 'convolution2DLayer',\n",
    "    'MaxPooling2D': 'maxPooling2DLayer'\n",
    "}\n",
    "\n",
    "layer_params_dict = {\n",
    "    'Dense': ['weights', 'activation'],\n",
    "    'Dropout': ['p'],\n",
    "    'Flatten': [],\n",
    "    'Embedding': ['weights'],\n",
    "    'BatchNormalization': ['weights', 'epsilon'],\n",
    "    'LeakyReLU': ['alpha'],\n",
    "    'PReLU': ['weights'],\n",
    "    'ParametricSoftplus': ['weights'],\n",
    "    'ThresholdedLinear': ['theta'],\n",
    "    'ThresholdedReLu': ['theta'],\n",
    "    'LSTM': ['weights', 'activation', 'inner_activation'],\n",
    "    'GRU': ['weights', 'activation', 'inner_activation'],\n",
    "    'JZS1': ['weights', 'activation', 'inner_activation'],\n",
    "    'JZS2': ['weights', 'activation', 'inner_activation'],\n",
    "    'JZS3': ['weights', 'activation', 'inner_activation'],\n",
    "    'Convolution2D': ['weights', 'nb_filter', 'stack_size', 'nb_row', 'nb_col', 'border_mode', 'subsample', 'activation'],\n",
    "    'MaxPooling2D': ['poolsize', 'stride', 'ignore_border']\n",
    "}\n",
    "\n",
    "layer_weights_dict = {\n",
    "    'Dense': ['W', 'b'],\n",
    "    'Embedding': ['E'],\n",
    "    'BatchNormalization': ['gamma', 'beta', 'mean', 'std'],\n",
    "    'PReLU': ['alphas'],\n",
    "    'ParametricSoftplus': ['alphas', 'betas'],\n",
    "    'LSTM': ['W_xi', 'W_hi', 'b_i', 'W_xc', 'W_hc', 'b_c', 'W_xf', 'W_hf', 'b_f', 'W_xo', 'W_ho', 'b_o'],\n",
    "    'GRU': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'JZS1': ['W_xz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS2': ['W_xz', 'W_hz', 'b_z', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS3': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'Convolution2D': ['W', 'b']\n",
    "}\n",
    "\n",
    "def serialize(model_json_file, weights_hdf5_file, save_filepath, compress):\n",
    "    with open(model_json_file, 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    weights_file = h5py.File(weights_hdf5_file, 'r')\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    num_activation_layers = 0\n",
    "    for k, layer in enumerate(model_metadata['layers']):\n",
    "        if layer['name'] == 'Activation':\n",
    "            num_activation_layers += 1\n",
    "            prev_layer_name = model_metadata['layers'][k-1]['name']\n",
    "            idx_activation = layer_params_dict[prev_layer_name].index('activation')\n",
    "            layers[k-num_activation_layers]['parameters'][idx_activation] = layer['activation']\n",
    "            continue\n",
    "\n",
    "        layer_params = []\n",
    "\n",
    "        for param in layer_params_dict[layer['name']]:\n",
    "            if param == 'weights':\n",
    "                layer_weights = list(weights_file.get('layer_{}'.format(k)))\n",
    "                weights = {}\n",
    "                weight_names = layer_weights_dict[layer['name']]\n",
    "                for name, w in zip(weight_names, layer_weights):\n",
    "                    weights[name] = weights_file.get('layer_{}/{}'.format(k, w)).value.tolist()\n",
    "                layer_params.append(weights)\n",
    "            else:\n",
    "                layer_params.append(layer[param])\n",
    "\n",
    "        layers.append({\n",
    "            'layerName': layer_name_dict[layer['name']],\n",
    "            'parameters': layer_params\n",
    "        })\n",
    "\n",
    "    if compress:\n",
    "        with gzip.open(save_filepath, 'wb') as f:\n",
    "            f.write(json.dumps(layers).encode('utf8'))\n",
    "    else:\n",
    "        with open(save_filepath, 'w') as f:\n",
    "            json.dump(layers, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "model_metadata = json.loads(model.to_json())\n",
    "\n",
    "with open('cifar10_cnn_keras_model.json', 'w') as f:\n",
    "    json.dump(model_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "serialize('cifar10_cnn_keras_model.json', \n",
    "          'cifar10_cnn_keras_weights.hdf5', \n",
    "          'cifar10_cnn_model_params.json.gz', \n",
    "          True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "randidx = np.random.randint(0, X_test.shape[0], size=1000)\n",
    "X_rand = X_test[randidx, :]\n",
    "y_rand = y_test[randidx]\n",
    "\n",
    "with gzip.open('sample_data.json.gz', 'wb') as f:\n",
    "    f.write(json.dumps({'data': X_rand.tolist(), 'labels': y_rand.tolist()}).encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.09631377e-16,   9.07704946e-23,   1.75124531e-12,\n",
       "          1.22915789e-15,   4.40595516e-09,   2.20104557e-08,\n",
       "          2.39443744e-18,   1.00000000e+00,   1.54725979e-22,\n",
       "          1.35819436e-15]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_rand[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
