{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 3: Tesla K80 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ParametricSoftplus\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 3, 32, 32)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "nb_classes = 10\n",
    "nb_epoch = 100\n",
    "\n",
    "img_channels = 3\n",
    "img_rows = 32\n",
    "img_cols = 32\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...1...2...3...4...5...6...7...8...9...10...11...12...13...14...15...16...17...18...19...20...21...22...23...24...X_train shape: (100000, 3, 32, 32)\n",
      "100000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False)\n",
    "\n",
    "batch = 0\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=2048):\n",
    "    print(batch, end='...', flush=True)\n",
    "    X_train = np.vstack((X_train, X_batch))\n",
    "    y_train = np.vstack((y_train, y_batch))\n",
    "    batch += 1\n",
    "    \n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train shape: (100000, 10)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "100000/100000 [==============================] - 57s - loss: 1.3785 - acc: 0.5029 - val_loss: 0.9185 - val_acc: 0.6753\n",
      "Epoch 00000: val_loss improved from inf to 0.91851, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 2/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.9605 - acc: 0.6646 - val_loss: 0.7146 - val_acc: 0.7512\n",
      "Epoch 00001: val_loss improved from 0.91851 to 0.71456, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 3/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.8401 - acc: 0.7077 - val_loss: 0.6169 - val_acc: 0.7833\n",
      "Epoch 00002: val_loss improved from 0.71456 to 0.61692, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 4/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.7738 - acc: 0.7325 - val_loss: 0.6005 - val_acc: 0.7899\n",
      "Epoch 00003: val_loss improved from 0.61692 to 0.60046, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 5/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.7306 - acc: 0.7467 - val_loss: 0.5599 - val_acc: 0.8122\n",
      "Epoch 00004: val_loss improved from 0.60046 to 0.55993, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 6/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.6954 - acc: 0.7588 - val_loss: 0.5413 - val_acc: 0.8226\n",
      "Epoch 00005: val_loss improved from 0.55993 to 0.54131, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 7/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.6749 - acc: 0.7681 - val_loss: 0.5293 - val_acc: 0.8203\n",
      "Epoch 00006: val_loss improved from 0.54131 to 0.52934, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 8/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.6569 - acc: 0.7736 - val_loss: 0.5511 - val_acc: 0.8160\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 9/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.6383 - acc: 0.7792 - val_loss: 0.5173 - val_acc: 0.8233\n",
      "Epoch 00008: val_loss improved from 0.52934 to 0.51727, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 10/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.6244 - acc: 0.7840 - val_loss: 0.5242 - val_acc: 0.8204\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 11/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.6099 - acc: 0.7903 - val_loss: 0.5120 - val_acc: 0.8284\n",
      "Epoch 00010: val_loss improved from 0.51727 to 0.51201, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 12/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5982 - acc: 0.7949 - val_loss: 0.4893 - val_acc: 0.8371\n",
      "Epoch 00011: val_loss improved from 0.51201 to 0.48930, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 13/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5902 - acc: 0.7984 - val_loss: 0.4714 - val_acc: 0.8431\n",
      "Epoch 00012: val_loss improved from 0.48930 to 0.47141, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 14/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5824 - acc: 0.8000 - val_loss: 0.4744 - val_acc: 0.8407\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 15/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5743 - acc: 0.8011 - val_loss: 0.4860 - val_acc: 0.8373\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 16/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5667 - acc: 0.8071 - val_loss: 0.4541 - val_acc: 0.8518\n",
      "Epoch 00015: val_loss improved from 0.47141 to 0.45413, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 17/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5671 - acc: 0.8049 - val_loss: 0.4734 - val_acc: 0.8504\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 18/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5563 - acc: 0.8097 - val_loss: 0.4494 - val_acc: 0.8479\n",
      "Epoch 00017: val_loss improved from 0.45413 to 0.44941, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 19/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5493 - acc: 0.8099 - val_loss: 0.4466 - val_acc: 0.8529\n",
      "Epoch 00018: val_loss improved from 0.44941 to 0.44660, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 20/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5451 - acc: 0.8139 - val_loss: 0.4624 - val_acc: 0.8462\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 21/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5402 - acc: 0.8146 - val_loss: 0.4799 - val_acc: 0.8388\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 22/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5386 - acc: 0.8161 - val_loss: 0.4483 - val_acc: 0.8558\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 23/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5337 - acc: 0.8186 - val_loss: 0.4543 - val_acc: 0.8514\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 24/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5297 - acc: 0.8180 - val_loss: 0.5325 - val_acc: 0.8241\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 25/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5292 - acc: 0.8177 - val_loss: 0.4438 - val_acc: 0.8535\n",
      "Epoch 00024: val_loss improved from 0.44660 to 0.44380, saving model to cifar10_cnn_keras_weights.hdf5\n",
      "Epoch 26/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5258 - acc: 0.8208 - val_loss: 0.4491 - val_acc: 0.8489\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 27/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5206 - acc: 0.8213 - val_loss: 0.4757 - val_acc: 0.8487\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 28/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5220 - acc: 0.8216 - val_loss: 0.4560 - val_acc: 0.8586\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 29/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5161 - acc: 0.8245 - val_loss: 0.4674 - val_acc: 0.8533\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 30/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5163 - acc: 0.8235 - val_loss: 0.4593 - val_acc: 0.8529\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 31/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5114 - acc: 0.8266 - val_loss: 0.4752 - val_acc: 0.8501\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 32/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5094 - acc: 0.8267 - val_loss: 0.4517 - val_acc: 0.8592\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 33/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5115 - acc: 0.8253 - val_loss: 0.4527 - val_acc: 0.8586\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 34/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5058 - acc: 0.8275 - val_loss: 0.4821 - val_acc: 0.8497\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 35/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5030 - acc: 0.8290 - val_loss: 0.4855 - val_acc: 0.8498\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 36/100\n",
      "100000/100000 [==============================] - 57s - loss: 0.5064 - acc: 0.8276 - val_loss: 0.4633 - val_acc: 0.8490\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d212a50f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='full',\n",
    "                        input_shape=(img_channels, img_rows, img_cols)))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='full'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', class_mode='categorical')\n",
    "    \n",
    "checkpointer = ModelCheckpoint(filepath='cifar10_cnn_keras_weights.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=batch_size, \n",
    "          nb_epoch=nb_epoch, \n",
    "          show_accuracy=True,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "layer_name_dict = {\n",
    "    'Dense': 'denseLayer',\n",
    "    'Dropout': 'dropoutLayer',\n",
    "    'Flatten': 'flattenLayer',\n",
    "    'Embedding': 'embeddingLayer',\n",
    "    'BatchNormalization': 'batchNormalizationLayer',\n",
    "    'LeakyReLU': 'leakyReLULayer',\n",
    "    'PReLU': 'parametricReLULayer',\n",
    "    'ParametricSoftplus': 'parametricSoftplusLayer',\n",
    "    'ThresholdedLinear': 'thresholdedLinearLayer',\n",
    "    'ThresholdedReLu': 'thresholdedReLuLayer',\n",
    "    'LSTM': 'rLSTMLayer',\n",
    "    'GRU': 'rGRULayer',\n",
    "    'JZS1': 'rJZS1Layer',\n",
    "    'JZS2': 'rJZS2Layer',\n",
    "    'JZS3': 'rJZS3Layer',\n",
    "    'Convolution2D': 'convolution2DLayer',\n",
    "    'MaxPooling2D': 'maxPooling2DLayer'\n",
    "}\n",
    "\n",
    "layer_params_dict = {\n",
    "    'Dense': ['weights', 'activation'],\n",
    "    'Dropout': ['p'],\n",
    "    'Flatten': [],\n",
    "    'Embedding': ['weights'],\n",
    "    'BatchNormalization': ['weights', 'epsilon'],\n",
    "    'LeakyReLU': ['alpha'],\n",
    "    'PReLU': ['weights'],\n",
    "    'ParametricSoftplus': ['weights'],\n",
    "    'ThresholdedLinear': ['theta'],\n",
    "    'ThresholdedReLu': ['theta'],\n",
    "    'LSTM': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'GRU': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS1': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS2': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS3': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'Convolution2D': ['weights', 'nb_filter', 'nb_row', 'nb_col', 'border_mode', 'subsample', 'activation'],\n",
    "    'MaxPooling2D': ['pool_size', 'stride', 'ignore_border']\n",
    "}\n",
    "\n",
    "layer_weights_dict = {\n",
    "    'Dense': ['W', 'b'],\n",
    "    'Embedding': ['E'],\n",
    "    'BatchNormalization': ['gamma', 'beta', 'mean', 'std'],\n",
    "    'PReLU': ['alphas'],\n",
    "    'ParametricSoftplus': ['alphas', 'betas'],\n",
    "    'LSTM': ['W_xi', 'W_hi', 'b_i', 'W_xc', 'W_hc', 'b_c', 'W_xf', 'W_hf', 'b_f', 'W_xo', 'W_ho', 'b_o'],\n",
    "    'GRU': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'JZS1': ['W_xz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS2': ['W_xz', 'W_hz', 'b_z', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS3': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'Convolution2D': ['W', 'b']\n",
    "}\n",
    "\n",
    "def serialize(model_json_file, weights_hdf5_file, save_filepath, compress):\n",
    "    with open(model_json_file, 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    weights_file = h5py.File(weights_hdf5_file, 'r')\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    num_activation_layers = 0\n",
    "    for k, layer in enumerate(model_metadata['layers']):\n",
    "        if layer['name'] == 'Activation':\n",
    "            num_activation_layers += 1\n",
    "            prev_layer_name = model_metadata['layers'][k-1]['name']\n",
    "            idx_activation = layer_params_dict[prev_layer_name].index('activation')\n",
    "            layers[k-num_activation_layers]['parameters'][idx_activation] = layer['activation']\n",
    "            continue\n",
    "\n",
    "        layer_params = []\n",
    "\n",
    "        for param in layer_params_dict[layer['name']]:\n",
    "            if param == 'weights':\n",
    "                layer_weights = list(weights_file.get('layer_{}'.format(k)))\n",
    "                weights = {}\n",
    "                weight_names = layer_weights_dict[layer['name']]\n",
    "                for name, w in zip(weight_names, layer_weights):\n",
    "                    weights[name] = weights_file.get('layer_{}/{}'.format(k, w)).value.tolist()\n",
    "                layer_params.append(weights)\n",
    "            else:\n",
    "                layer_params.append(layer[param])\n",
    "\n",
    "        layers.append({\n",
    "            'layerName': layer_name_dict[layer['name']],\n",
    "            'parameters': layer_params\n",
    "        })\n",
    "\n",
    "    if compress:\n",
    "        with gzip.open(save_filepath, 'wb') as f:\n",
    "            f.write(json.dumps(layers).encode('utf8'))\n",
    "    else:\n",
    "        with open(save_filepath, 'w') as f:\n",
    "            json.dump(layers, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "model_metadata = json.loads(model.to_json())\n",
    "\n",
    "with open('cifar10_cnn_keras_model.json', 'w') as f:\n",
    "    json.dump(model_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_mode': 'categorical',\n",
       " 'layers': [{'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'full',\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_shape': [3, 32, 32],\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 32,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'valid',\n",
       "   'init': 'glorot_uniform',\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 32,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'ignore_border': True,\n",
       "   'name': 'MaxPooling2D',\n",
       "   'pool_size': [2, 2],\n",
       "   'stride': [2, 2]},\n",
       "  {'name': 'Dropout', 'p': 0.2},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'full',\n",
       "   'init': 'glorot_uniform',\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 64,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'valid',\n",
       "   'init': 'glorot_uniform',\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 64,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'ignore_border': True,\n",
       "   'name': 'MaxPooling2D',\n",
       "   'pool_size': [2, 2],\n",
       "   'stride': [2, 2]},\n",
       "  {'name': 'Dropout', 'p': 0.3},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'full',\n",
       "   'init': 'glorot_uniform',\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 128,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'border_mode': 'valid',\n",
       "   'init': 'glorot_uniform',\n",
       "   'name': 'Convolution2D',\n",
       "   'nb_col': 3,\n",
       "   'nb_filter': 128,\n",
       "   'nb_row': 3,\n",
       "   'subsample': [1, 1]},\n",
       "  {'alpha': 0.2, 'name': 'LeakyReLU'},\n",
       "  {'ignore_border': True,\n",
       "   'name': 'MaxPooling2D',\n",
       "   'pool_size': [2, 2],\n",
       "   'stride': [2, 2]},\n",
       "  {'name': 'Dropout', 'p': 0.4},\n",
       "  {'name': 'Flatten'},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 512},\n",
       "  {'activation': 'relu', 'beta': 0.1, 'name': 'Activation', 'target': 0},\n",
       "  {'name': 'Dropout', 'p': 0.5},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 10},\n",
       "  {'activation': 'softmax', 'beta': 0.1, 'name': 'Activation', 'target': 0}],\n",
       " 'loss': 'categorical_crossentropy',\n",
       " 'name': 'Sequential',\n",
       " 'optimizer': {'beta_1': 0.9,\n",
       "  'beta_2': 0.999,\n",
       "  'epsilon': 1e-08,\n",
       "  'lr': 0.0010000000474974513,\n",
       "  'name': 'Adam'},\n",
       " 'theano_mode': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "serialize('cifar10_cnn_keras_model.json', \n",
    "          'cifar10_cnn_keras_weights.hdf5', \n",
    "          'cifar10_cnn_model_params.json.gz', \n",
    "          True)\n",
    "serialize('cifar10_cnn_keras_model.json', \n",
    "          'cifar10_cnn_keras_weights.hdf5', \n",
    "          'cifar10_cnn_model_params.json', \n",
    "          False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "randidx = np.random.randint(0, X_test.shape[0], size=500)\n",
    "X_rand = X_test[randidx, :]\n",
    "y_rand = y_test[randidx]\n",
    "\n",
    "with gzip.open('sample_data.json.gz', 'wb') as f:\n",
    "    f.write(json.dumps({'data': X_rand.tolist(), 'labels': y_rand.tolist()}).encode('utf8'))\n",
    "with open('sample_data.json', 'w') as f:\n",
    "    json.dump({'data': X_rand.tolist(), 'labels': y_rand.tolist()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.96 ms, sys: 0 ns, total: 3.96 ms\n",
      "Wall time: 3.36 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  3.06449920e-06,   9.98904705e-01,   8.34237124e-09,\n",
       "          3.80483201e-08,   6.29814326e-11,   2.97701447e-10,\n",
       "          9.51078718e-08,   1.12220067e-09,   8.56426632e-05,\n",
       "          1.00651255e-03]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.predict(X_rand[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
