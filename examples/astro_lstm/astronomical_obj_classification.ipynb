{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "res = requests.get('https://en.wikipedia.org/wiki/Lists_of_stars_by_constellation')\n",
    "soup = BeautifulSoup(res.text)\n",
    "for link in soup.find_all('a'):\n",
    "    try:\n",
    "        if link['href'].startswith('/wiki/List_of_stars_in_'):\n",
    "            links.append('https://en.wikipedia.org' + link['href'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    res = requests.get(link)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    for row in soup.table.children:\n",
    "        if row.name == 'tr':\n",
    "            for rowc in row.children:\n",
    "                if rowc.name == 'td':\n",
    "                    print(row.td.text)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = ['https://en.wikipedia.org/wiki/List_of_exoplanets_detected_by_radial_velocity',\n",
    "         'https://en.wikipedia.org/wiki/List_of_transiting_exoplanets',\n",
    "         'https://en.wikipedia.org/wiki/List_of_exoplanets_detected_by_microlensing',\n",
    "         'https://en.wikipedia.org/wiki/List_of_directly_imaged_exoplanets',\n",
    "         'https://en.wikipedia.org/wiki/List_of_exoplanets_detected_by_timing']\n",
    "res = requests.get(links[4])\n",
    "soup = BeautifulSoup(res.text)\n",
    "for row in soup.find_all('table')[0].children:\n",
    "    if row.name == 'tr':\n",
    "        for rowc in row.children:\n",
    "            if rowc.name == 'td':\n",
    "                print(row.td.text)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "\n",
    "names = []\n",
    "labels = []\n",
    "\n",
    "with codecs.open('astronomical_objects.csv', 'r', 'utf8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        names.append(line[0])\n",
    "        labels.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rid of duplicates\n",
    "\n",
    "names = [name.replace('\\n', ' ') for name in names]\n",
    "\n",
    "objs = []\n",
    "for obj in list(zip(names, labels)):\n",
    "    if len(obj[0].strip()) != 0 and obj not in objs:\n",
    "        objs.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14215"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = []\n",
    "labels = []\n",
    "\n",
    "for n, l in objs:\n",
    "    names.append(n)\n",
    "    labels.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_list = list(set(''.join(names)))\n",
    "labels_list = list(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(char_list))\n",
    "indices_char = dict((i, c) for i, c in enumerate(char_list))\n",
    "label_indices = dict((l, i) for i, l in enumerate(labels_list))\n",
    "indices_label = dict((i, l) for i, l in enumerate(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comet': 0, 'asteroid': 1, 'quasar': 2, 'galaxy': 5, 'star': 4, 'planet': 3}\n"
     ]
    }
   ],
   "source": [
    "print(label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 0\n",
    "for n in names:\n",
    "    if len(n) > MAX_LENGTH:\n",
    "        MAX_LENGTH = len(n)\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if MAX_LENGTH < 50:\n",
    "    MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 3: Tesla K80 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, JZS1, JZS2, JZS3\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_to_char_seq(name):\n",
    "    name_chars = list(name)\n",
    "    name_chars_indices = list(map(lambda char: char_indices[char], name_chars))\n",
    "    return sequence.pad_sequences([name_chars_indices], maxlen=MAX_LENGTH)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14215, 50) (14215, 6)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for n, l in zip(names, labels):\n",
    "    X.append(name_to_char_seq(n))\n",
    "    y.append(label_indices[l])\n",
    "    \n",
    "X = np.array(X).astype(np.uint8)\n",
    "y = np_utils.to_categorical(np.array(y)).astype(np.bool)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12793 samples, validate on 1422 samples\n",
      "Epoch 1/100\n",
      "12793/12793 [==============================] - 24s - loss: 0.4070 - acc: 0.8983 - val_loss: 0.1755 - val_acc: 0.9430\n",
      "Epoch 00000: val_loss improved from inf to 0.17550, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 2/100\n",
      "12793/12793 [==============================] - 24s - loss: 0.1495 - acc: 0.9552 - val_loss: 0.1259 - val_acc: 0.9620\n",
      "Epoch 00001: val_loss improved from 0.17550 to 0.12592, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 3/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.1122 - acc: 0.9671 - val_loss: 0.1063 - val_acc: 0.9662\n",
      "Epoch 00002: val_loss improved from 0.12592 to 0.10634, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 4/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0902 - acc: 0.9723 - val_loss: 0.0910 - val_acc: 0.9698\n",
      "Epoch 00003: val_loss improved from 0.10634 to 0.09104, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 5/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0741 - acc: 0.9764 - val_loss: 0.0746 - val_acc: 0.9761\n",
      "Epoch 00004: val_loss improved from 0.09104 to 0.07461, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 6/100\n",
      "12793/12793 [==============================] - 24s - loss: 0.0654 - acc: 0.9796 - val_loss: 0.0713 - val_acc: 0.9775\n",
      "Epoch 00005: val_loss improved from 0.07461 to 0.07127, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 7/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0606 - acc: 0.9815 - val_loss: 0.0615 - val_acc: 0.9810\n",
      "Epoch 00006: val_loss improved from 0.07127 to 0.06148, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 8/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0518 - acc: 0.9859 - val_loss: 0.0562 - val_acc: 0.9852\n",
      "Epoch 00007: val_loss improved from 0.06148 to 0.05622, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 9/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0455 - acc: 0.9875 - val_loss: 0.0542 - val_acc: 0.9831\n",
      "Epoch 00008: val_loss improved from 0.05622 to 0.05418, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 10/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0427 - acc: 0.9875 - val_loss: 0.0532 - val_acc: 0.9845\n",
      "Epoch 00009: val_loss improved from 0.05418 to 0.05323, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 11/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0366 - acc: 0.9898 - val_loss: 0.0436 - val_acc: 0.9887\n",
      "Epoch 00010: val_loss improved from 0.05323 to 0.04361, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 12/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0369 - acc: 0.9899 - val_loss: 0.0476 - val_acc: 0.9873\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 13/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0337 - acc: 0.9901 - val_loss: 0.0390 - val_acc: 0.9909\n",
      "Epoch 00012: val_loss improved from 0.04361 to 0.03902, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 14/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0261 - acc: 0.9932 - val_loss: 0.0361 - val_acc: 0.9902\n",
      "Epoch 00013: val_loss improved from 0.03902 to 0.03606, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 15/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0256 - acc: 0.9936 - val_loss: 0.0477 - val_acc: 0.9852\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 16/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0306 - acc: 0.9902 - val_loss: 0.0403 - val_acc: 0.9887\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 17/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0222 - acc: 0.9942 - val_loss: 0.0288 - val_acc: 0.9923\n",
      "Epoch 00016: val_loss improved from 0.03606 to 0.02883, saving model to astro_lstm_keras_weights.hdf5\n",
      "Epoch 18/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0221 - acc: 0.9936 - val_loss: 0.0321 - val_acc: 0.9916\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 19/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0202 - acc: 0.9949 - val_loss: 0.0328 - val_acc: 0.9909\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 20/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0167 - acc: 0.9962 - val_loss: 0.0342 - val_acc: 0.9909\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 21/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0132 - acc: 0.9968 - val_loss: 0.0354 - val_acc: 0.9887\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 22/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0144 - acc: 0.9962 - val_loss: 0.0331 - val_acc: 0.9909\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 23/100\n",
      "12793/12793 [==============================] - 23s - loss: 0.0205 - acc: 0.9951 - val_loss: 0.0372 - val_acc: 0.9909\n",
      "Epoch 00022: early stopping\n",
      "Epoch 00022: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc499fc15f8>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(char_list), 64, input_length=MAX_LENGTH, mask_zero=True))\n",
    "model.add(LSTM(64, init='glorot_uniform', inner_init='orthogonal',\n",
    "               activation='tanh', inner_activation='hard_sigmoid', \n",
    "               return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(labels_list)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', class_mode='categorical')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='astro_lstm_keras_weights.hdf5', \n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "model.fit(X, y, \n",
    "          batch_size=batch_size, \n",
    "          nb_epoch=nb_epoch, \n",
    "          show_accuracy=True,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      comet       1.00      1.00      1.00      2984\n",
      "   asteroid       0.98      0.98      0.98       222\n",
      "     quasar       0.94      0.89      0.92       208\n",
      "     planet       1.00      0.98      0.99       325\n",
      "       star       1.00      1.00      1.00     10219\n",
      "     galaxy       0.94      0.95      0.95       257\n",
      "\n",
      "avg / total       1.00      1.00      1.00     14215\n",
      "\n",
      "\n",
      "[[ 2982     0     0     0     0     2]\n",
      " [    0   218     0     0     3     1]\n",
      " [    0     0   185     0    17     6]\n",
      " [    0     1     0   319     5     0]\n",
      " [    0     2    10     1 10200     6]\n",
      " [    0     1     1     0    10   245]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model.load_weights('astro_lstm_keras_weights.hdf5')\n",
    "preds = model.predict_classes(X, batch_size=64, verbose=0)\n",
    "\n",
    "print('')\n",
    "print(classification_report(np.argmax(y, axis=1), preds, target_names=labels_list))\n",
    "print('')\n",
    "print(confusion_matrix(np.argmax(y, axis=1), preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "layer_name_dict = {\n",
    "    'Dense': 'denseLayer',\n",
    "    'Dropout': 'dropoutLayer',\n",
    "    'Flatten': 'flattenLayer',\n",
    "    'Embedding': 'embeddingLayer',\n",
    "    'BatchNormalization': 'batchNormalizationLayer',\n",
    "    'LeakyReLU': 'leakyReLULayer',\n",
    "    'PReLU': 'parametricReLULayer',\n",
    "    'ParametricSoftplus': 'parametricSoftplusLayer',\n",
    "    'ThresholdedLinear': 'thresholdedLinearLayer',\n",
    "    'ThresholdedReLu': 'thresholdedReLuLayer',\n",
    "    'LSTM': 'rLSTMLayer',\n",
    "    'GRU': 'rGRULayer',\n",
    "    'JZS1': 'rJZS1Layer',\n",
    "    'JZS2': 'rJZS2Layer',\n",
    "    'JZS3': 'rJZS3Layer',\n",
    "    'Convolution2D': 'convolution2DLayer',\n",
    "    'MaxPooling2D': 'maxPooling2DLayer'\n",
    "}\n",
    "\n",
    "layer_params_dict = {\n",
    "    'Dense': ['weights', 'activation'],\n",
    "    'Dropout': ['p'],\n",
    "    'Flatten': [],\n",
    "    'Embedding': ['weights'],\n",
    "    'BatchNormalization': ['weights', 'epsilon'],\n",
    "    'LeakyReLU': ['alpha'],\n",
    "    'PReLU': ['weights'],\n",
    "    'ParametricSoftplus': ['weights'],\n",
    "    'ThresholdedLinear': ['theta'],\n",
    "    'ThresholdedReLu': ['theta'],\n",
    "    'LSTM': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'GRU': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS1': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS2': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'JZS3': ['weights', 'activation', 'inner_activation', 'return_sequences'],\n",
    "    'Convolution2D': ['weights', 'nb_filter', 'nb_row', 'nb_col', 'border_mode', 'subsample', 'activation'],\n",
    "    'MaxPooling2D': ['pool_size', 'stride', 'ignore_border']\n",
    "}\n",
    "\n",
    "layer_weights_dict = {\n",
    "    'Dense': ['W', 'b'],\n",
    "    'Embedding': ['E'],\n",
    "    'BatchNormalization': ['gamma', 'beta', 'mean', 'std'],\n",
    "    'PReLU': ['alphas'],\n",
    "    'ParametricSoftplus': ['alphas', 'betas'],\n",
    "    'LSTM': ['W_xi', 'W_hi', 'b_i', 'W_xc', 'W_hc', 'b_c', 'W_xf', 'W_hf', 'b_f', 'W_xo', 'W_ho', 'b_o'],\n",
    "    'GRU': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'JZS1': ['W_xz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS2': ['W_xz', 'W_hz', 'b_z', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h', 'Pmat'],\n",
    "    'JZS3': ['W_xz', 'W_hz', 'b_z', 'W_xr', 'W_hr', 'b_r', 'W_xh', 'W_hh', 'b_h'],\n",
    "    'Convolution2D': ['W', 'b']\n",
    "}\n",
    "\n",
    "def serialize(model_json_file, weights_hdf5_file, save_filepath, compress):\n",
    "    with open(model_json_file, 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    weights_file = h5py.File(weights_hdf5_file, 'r')\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    num_activation_layers = 0\n",
    "    for k, layer in enumerate(model_metadata['layers']):\n",
    "        if layer['name'] == 'Activation':\n",
    "            num_activation_layers += 1\n",
    "            prev_layer_name = model_metadata['layers'][k-1]['name']\n",
    "            idx_activation = layer_params_dict[prev_layer_name].index('activation')\n",
    "            layers[k-num_activation_layers]['parameters'][idx_activation] = layer['activation']\n",
    "            continue\n",
    "\n",
    "        layer_params = []\n",
    "\n",
    "        for param in layer_params_dict[layer['name']]:\n",
    "            if param == 'weights':\n",
    "                layer_weights = list(weights_file.get('layer_{}'.format(k)))\n",
    "                weights = {}\n",
    "                weight_names = layer_weights_dict[layer['name']]\n",
    "                for name, w in zip(weight_names, layer_weights):\n",
    "                    weights[name] = weights_file.get('layer_{}/{}'.format(k, w)).value.tolist()\n",
    "                layer_params.append(weights)\n",
    "            else:\n",
    "                layer_params.append(layer[param])\n",
    "\n",
    "        layers.append({\n",
    "            'layerName': layer_name_dict[layer['name']],\n",
    "            'parameters': layer_params\n",
    "        })\n",
    "\n",
    "    if compress:\n",
    "        with gzip.open(save_filepath, 'wb') as f:\n",
    "            f.write(json.dumps(layers).encode('utf8'))\n",
    "    else:\n",
    "        with open(save_filepath, 'w') as f:\n",
    "            json.dump(layers, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "model_metadata = json.loads(model.to_json())\n",
    "\n",
    "with open('astro_lstm_keras_model.json', 'w') as f:\n",
    "    json.dump(model_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_mode': 'categorical',\n",
       " 'layers': [{'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activity_regularizer': None,\n",
       "   'init': 'uniform',\n",
       "   'input_dim': 112,\n",
       "   'input_length': 50,\n",
       "   'input_shape': [112],\n",
       "   'mask_zero': True,\n",
       "   'name': 'Embedding',\n",
       "   'output_dim': 64},\n",
       "  {'activation': 'tanh',\n",
       "   'forget_bias_init': 'one',\n",
       "   'init': 'glorot_uniform',\n",
       "   'inner_activation': 'hard_sigmoid',\n",
       "   'inner_init': 'orthogonal',\n",
       "   'input_dim': None,\n",
       "   'input_length': None,\n",
       "   'name': 'LSTM',\n",
       "   'output_dim': 64,\n",
       "   'return_sequences': False,\n",
       "   'truncate_gradient': -1},\n",
       "  {'name': 'Dropout', 'p': 0.5},\n",
       "  {'W_constraint': None,\n",
       "   'W_regularizer': None,\n",
       "   'activation': 'linear',\n",
       "   'activity_regularizer': None,\n",
       "   'b_constraint': None,\n",
       "   'b_regularizer': None,\n",
       "   'init': 'glorot_uniform',\n",
       "   'input_dim': None,\n",
       "   'name': 'Dense',\n",
       "   'output_dim': 6},\n",
       "  {'activation': 'softmax', 'beta': 0.1, 'name': 'Activation', 'target': 0}],\n",
       " 'loss': 'categorical_crossentropy',\n",
       " 'name': 'Sequential',\n",
       " 'optimizer': {'beta_1': 0.9,\n",
       "  'beta_2': 0.999,\n",
       "  'epsilon': 1e-08,\n",
       "  'lr': 0.0010000000474974513,\n",
       "  'name': 'Adam'},\n",
       " 'theano_mode': None}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serialize('astro_lstm_keras_model.json', \n",
    "          'astro_lstm_keras_weights.hdf5', \n",
    "          'astro_lstm_model_params.json.gz', \n",
    "          True)\n",
    "serialize('astro_lstm_keras_model.json', \n",
    "          'astro_lstm_keras_weights.hdf5', \n",
    "          'astro_lstm_model_params.json', \n",
    "          False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "\n",
    "indices = []\n",
    "for i in range(np.max(y)):\n",
    "    indices.append(np.random.choice(np.where(y == i)[0], 100))\n",
    "indices = np.concatenate(tuple(indices))\n",
    "np.random.shuffle(indices)\n",
    "indices = indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names_rand = []\n",
    "labels_rand = []\n",
    "for i in indices:\n",
    "    names_rand.append(names[i])\n",
    "    labels_rand.append(labels[i])\n",
    "\n",
    "with gzip.open('astro_lstm_sample_data.json.gz', 'wb') as f:\n",
    "    f.write(json.dumps({'names': names_rand, 'labels': labels_rand}).encode('utf8'))\n",
    "with open('astro_lstm_sample_data.json', 'w') as f:\n",
    "    json.dump({'names': names_rand, 'labels': labels_rand}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 ms, sys: 28 ms, total: 43.7 ms\n",
      "Wall time: 43 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.50226856e-06,   5.04170603e-05,   1.35677372e-04,\n",
       "          1.68430142e-05,   9.99785244e-01,   1.03284810e-05]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.predict(X[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
